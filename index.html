<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 19px;
    font-weight: 1000
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 800
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  </style>
  <link rel="icon" type="image/png" href="images/star_thumbnail.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Shao-Yu (Becca) Chang</title>
  <meta name="Shao-Yu Chang's Homepage" http-equiv="Content-Type" content="Shao-Yu Chang's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <!-- <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-64069893-1', 'auto');
    ga('send', 'pageview');
  </script> -->
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
<table width="1000" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Shao-Yu (Becca) Chang</pageheading><br>
    <b>email</b>: shaochang -at- cs.stonybrook.edu
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
  </p>

  <tr>
    <td width="35%" valign="top"><a href="images/Becca.jpg"><img src="images/Becca.jpg" width="100%" style="border-radius:10px"></a>
      <p align=center>
          | <a href="https://drive.google.com/file/d/1r0j276P7zFUOOD701fwr69YFjsVm_jYT/view?usp=sharing">CV</a> |
          <a href="https://github.com/shaoyuc3">Github</a> |
      </p>
    </td>

    <td width="68%" valign="top" align="justify">
        <p>
          I am a first-year CS Ph.D. student at <a href="https://www.cs.stonybrook.edu/">Stony Brook University</a> working with Prof. <a href="https://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a> at <a href="https://www3.cs.stonybrook.edu/~cvl/">CVLab</a>.
          My research interests lie in the field of Computer Vision, with a focus on generative modeling and visual synthesis.
          <br>
          <br>
          Previously, I was a Research Assistant in <a href="https://homepage.iis.sinica.edu.tw/~liutyng/">CVML Lab</a> at <a href="https://www.iis.sinica.edu.tw/en/index.html">Academia Sinica</a> working with Prof. <a href="https://homepage.iis.sinica.edu.tw/pages/liutyng/index_en.html">Tyng-Luh Liu</a>
          and earned my Master's degree in Applied Mathematics with a specialization in Computational Science and Engineering at <a href="https://illinois.edu/">UIUC</a>, where I worked with Prof. <a href="http://luthuli.cs.uiuc.edu/~daf/">David Forsyth</a>.
          <br>
          <br>
          <b>
          I am actively seeking for a research internship for 2025 Summer.
          Please feel free to reach out to me if you believe I am a good fit for your research team!
          <b>
        </p>
        </td>
  </tr>
</table>


<!-- =================== Experience =================== -->
<table width="100%" align="center" style="margin-left:10px" cellspacing="0" cellpadding="0" border="0">
    <tr>
        <th width="16.6%" valign="top" align="center">
        <img src="images/sbu_logo.png" alt="sym" width="35%"></a>
        <p style="line-height:1.3; font-size:12pt">SBU<br>Ph.D. in CS<br>Aug. 24 - Present</p>
        </th>
      
        <th width="16.6%" valign="top" align="center">
        <img src="images/iis_logo.png" alt="sym" width="35%"></a>
        <p style="line-height:1.3; font-size:12pt">Academia Sinica<br>Research Assistant<br>Jul. 23 - May. 24</p>
        </th>
  
        <th width="16.6%" valign="top" align="center">
        <img src="images/uiuc_logo.png" alt="sym" width="24%"></a>
        <p style="line-height:1.3; font-size:12pt">UIUC<br>MS in Applied Math<br>Aug. 21 - May. 23</p>
        </th>

        <th width="16.6%" valign="top" align="center">
        <img src="images/nctu_icon.png" alt="sym" width="35%"></a>
        <p style="line-height:1.3; font-size:12pt">NCTU<br>BS in Applied Math<br>Research Assistant</p>
        </th>
    </tr>
</table>

<br>

  <!-- =================== Updates =================== -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="8">
  <tr><td>
    <sectionheading>&nbsp;&nbsp;Updates</sectionheading>
    <ul>
      <li> [08/2024] Start my Ph.D. at SBU CS!</li>
    </ul>
  </td></tr>
</table>

<br>
  
<!-- =================== Publications =================== -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

    <tr>
        <td width="33%" valign="top" align="center">
            <img src="images/acdg-vton4.png" alt="sym" width="300" height="180" style="border-radius:15px">
        </td>

        <td width="67%" valign="top">
            <p>
                <a href="#" id="ACDG-VTON"></a>
                <heading>ACDG-VTON: Accurate and Contained Diffusion Generation for Virtual Try-On</heading></a>
                <br> 
                Jeffrey Zhang, Kedan Li, <u><b>Shao-Yu Chang</b></u>, David Forsyth
                <br>
                ArXiv preprint
                <br>
            </p>
    
            <div class="poster" id="ACDG-VTON">
            | <a href="javascript:toggleblock('ACDG-VTON_abs')">abstract</a> | <a href="https://arxiv.org/abs/2403.13951">arXiv</a> |
    
            <p align="justify">
                <i id='ACDG-VTON_abs'>
                  Virtual Try-on (VTON) involves generating images of a person wearing selected garments.
                  Diffusion-based methods, in particular, can create high-quality images, but they struggle to maintain the identities of the input garments.
                  We identified this problem stems from the specifics in the training formulation for diffusion.
                  To address this, we propose a unique training scheme that limits the scope in which diffusion is trained.
                  We use a control image that perfectly aligns with the target image during training.
                  In turn, this accurately preserves garment details during inference.
                  We demonstrate our method not only effectively conserves garment details but also allows for layering, styling, and shoe try-on.
                  Our method runs multi-garment try-on in a single inference cycle and can support high-quality zoomed-in generations without training in higher resolutions.
                  Finally, we show our method surpasses prior methods in accuracy and quality.
                </i>
            </p>
            </div>
        </td>
    </tr>

    <tr>
        <td width="33%" valign="top" align="center">
            <img src="images/diffusionAtlas.gif" alt="sym" width="300" height="180" style="border-radius:15px">
        </td>

        <td width="67%" valign="top">
            <p>
                <a href="#" id="DiffusionAtlas"></a>
                <heading>DiffusionAtlas: High-fidelity Consistent Diffusion Video Editing</heading></a>
                <br>
                <u><b>Shao-Yu Chang</b></u>, Hwann-Tzong Chen, Tyng-Luh Liu
                <br>
                ArXiv preprint
                <br>
            </p>
    
            <div class="poster" id="DiffusionAtlas">
            | <a href="javascript:toggleblock('DiffusionAtlas_abs')">abstract</a> |
            <a href="https://diffusionatlas.github.io/">project page</a> | <a href="https://arxiv.org/abs/2312.03772">arXiv</a> |
    
            <p align="justify">
                <i id='DiffusionAtlas_abs'>
                  We present a diffusion-based video editing framework, namely DiffusionAtlas, which can achieve both frame consistency and high fidelity in editing video object appearance.
                  Despite the success in image editing, diffusion models still encounter significant hindrances when it comes to video editing due to the challenge of maintaining spatiotemporal consistency in the object's appearance across frames.
                  On the other hand, atlas-based techniques allow propagating edits on the layered representations consistently back to frames.
                  However, they often struggle to create editing effects that adhere correctly to the user-provided textual or visual conditions due to the limitation of editing the texture atlas on a fixed UV mapping field.
                  Our method leverages a visual-textual diffusion model to edit objects directly on the diffusion atlases, ensuring coherent object identity across frames.
                  We design a loss term with atlas-based constraints and build a pretrained text-driven diffusion model as pixel-wise guidance for refining shape distortions and correcting texture deviations.
                  Qualitative and quantitative experiments show that our method outperforms state-of-the-art methods in achieving consistent high-fidelity video-object editing.
                </i>
            </p>
            </div>
        </td>
    </tr>

    <tr>
        <td width="33%" valign="top" align="center">
            <!-- <a href="#">
            <img src="images/iscas20.png" alt="sym" width="100%" height="14%" style="border-radius:15px"></a> -->
            <img src="images/initialization.png" alt="sym" width="300" height="200" style="border-radius:15px">
        </td>

        <td width="67%" valign="top">
            <p>
                <a href="#" id="initialization"></a>
                <heading>Preserving Image Properties Through Initializations in Diffusion Models</heading></a>
                <br>
                Jeffrey Zhang, <u><b>Shao-Yu Chang</b></u>, Kedan Li, David Forsyth
                <br>
                WACV 2024
                <br>
            </p>
    
            <div class="poster" id="initialization">
            | <a href="javascript:toggleblock('initialization_abs')">abstract</a> |
            <a href="https://jeffz0.github.io/StableDiffusionInitialization/">project page</a> |
            <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Preserving_Image_Properties_Through_Initializations_in_Diffusion_Models_WACV_2024_paper.pdf">paper</a> |
    
            <p align="justify">
                <i id='initialization_abs'>
                  Retail photography imposes specific requirements on images.
                  For instance, images may need uniform background colors, consistent model poses, centered products, and consistent lighting.
                  Minor deviations from these standards impact a site's aesthetic appeal, making the images unsuitable for use.
                  We show that Stable Diffusion methods, as currently applied, do not respect these requirements.
                  The usual practice of training the denoiser with a very noisy image and starting inference with a sample of pure noise leads to inconsistent generated images during inference.
                  This inconsistency occurs because it is easy to tell the difference between samples of the training and inference distributions.
                  As a result, a network trained with centered retail product images with uniform backgrounds generates images with erratic backgrounds.
                  The problem is easily fixed by initializing inference with samples from an approximation of noisy images.
                  However, in using such an approximation, the joint distribution of text and noisy image at inference time still slightly differs from that at training time.
                  This discrepancy is corrected by training the network with samples from the approximate noisy image distribution. Extensive experiments on real application data show significant qualitative and quantitative improvements in performance from adopting these procedures.
                  Finally, our procedure can interact well with other control-based methods to further enhance the controllability of diffusion-based methods.
                </i>
            </p>
            </div>
        </td>
    </tr>
  
    <tr>
        <td width="33%" valign="top" align="center">
            <!-- <a href="#">
            <img src="images/iscas20.png" alt="sym" width="100%" height="14%" style="border-radius:15px"></a> -->
            <img src="images/controllable2.png" alt="sym" width="400" height="180" style="border-radius:15px">
        </td>

        <td width="67%" valign="top">
            <p>
                <a href="#" id="controllable"></a>
                <heading>Controlling Virtual Try-on Pipeline Through Rendering Policies</heading></a>
                <br>
                Kedan Li, Jeffrey Zhang, <u><b>Shao-Yu Chang</b></u>, David Forsyth
                <br>
                WACV 2024
                <br>
            </p>
    
            <div class="poster" id="controllable">
            | <a href="javascript:toggleblock('controllable_abs')">abstract</a> |
            <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Li_Controlling_Virtual_Try-On_Pipeline_Through_Rendering_Policies_WACV_2024_paper.pdf">paper</a> |
    
            <p align="justify">
                <i id='controllable_abs'>
                  This paper shows how to impose rendering policies on a virtual try-on (VTON) pipeline. Our rendering policies are lightweight procedural descriptions of how the pipeline should render outfits or render particular types of garments.
                  Our policies are procedural expressions describing offsets to the control points for each set of garment types.
                  The policies are easily authored and are generalizable to any outfit composed of garments of similar types.
                  We describe a VTON pipeline that accepts our policies to modify garment drapes and produce high-quality try-on images with garment attributes preserved.
                  <br>
                  Layered outfits are a particular challenge to VTON systems because learning to coordinate warps between multiple garments so that nothing sticks out is difficult.
                  Our rendering policies offer a lightweight and effective procedure to achieve this coordination, while also allowing precise manipulation of drape.
                  Drape describes the way in which a garment is worn (for example, a shirt could be tucked or untucked).
                  <br>
                  Quantitative and qualitative evaluations demonstrate that our method allows effective manipulation of drape and produces significant measurable improvements in rendering quality for complicated layering interactions.
                </i>
            </p>
            </div>
        </td>
    </tr>

    <tr>
        <td width="33%" valign="top" align="center">
            <img src="images/handwriting.png" alt="sym" width="300" height="150" style="border-radius:15px">
        </td>

        <td width="67%" valign="top">
            <p>
                <a href="#" id="handwriting"></a>
                <heading>Comparing Handwriting Fluency in English Language Teaching Using Computer Vision Techniques</heading></a>
                <br>
                Chuan-Wei Syu, <u><b>Shao-Yu Chang</b></u>, Chi-Cheng Chang.
                <br>
                ICITL 2023
                <br>
            </p>
    
            <div class="poster" id="handwriting">
            | <a href="javascript:toggleblock('handwriting_abs')">abstract</a> |
            <a href="https://link.springer.com/chapter/10.1007/978-3-031-40113-8_5">paper</a> |
    
            <p align="justify">
                <i id='handwriting_abs'>
                  Educational materials play a vital role in effectively conveying information to learners, with the readability and legibility of written text serving as crucial factors.
                  This study investigates the influence of font selection on educational materials and explores the relationship between handwriting fluency and cognitive load.
                  By identifying challenges in written expression, such as reduced working memory capacity, text organization difficulties, and content recall issues, the study sheds light on the significance of neat handwriting.
                  The research emphasizes the relevance of neat handwriting in critical examinations, including college entrance exams, academic English exams, and job interviews, where the fluency of one’s handwriting can impact the decision-making process of interviewers.
                  This highlights the value of handwriting fluency beyond educational contexts.
                  Advancements in computer science and machine vision present new opportunities for automating font evaluation and selection.
                  By employing machine vision algorithms to objectively analyze visual features of fonts, such as serifs, stroke width, and character spacing, the legibility and readability of fonts used in English language teaching materials are assessed.
                  In this study, machine vision techniques are applied to score fonts used in educational materials.
                  The OpenCV computer vision library is utilized to extract visual features of fonts from images, enabling the analysis of their legibility and readability.
                  The primary objective is to provide educators with an automated and objective tool for scoring handwriting, reducing visual fatigue, and ensuring impartial evaluations.
                  This research contributes to enhancing the quality of educational materials and provides valuable insights for educators, researchers, and font designers.
                </i>
            </p>
            </div>
        </td>
    </tr>

    <tr>
        <td width="33%" valign="top" align="center">
            <img src="images/siamcse21.png" alt="sym" width="300" height="200" style="border-radius:15px">
        </td>

        <td width="67%" valign="top">
            <p>
                <a href="https://meetings.siam.org/sess/dsp_talk.cfm?p=109897">
                <heading>Classification of Satellite Images with Spectral Indices</heading></a>
                <br>
                <u><b>Shao-Yu Chang</b></u>, Chin-Tien Wu
                <br>
                SIAM CSE 2021 (Poster Presentation)
                <br>
            </p>
    
            <div class="poster" id="siamcse21">
            | <a href="javascript:toggleblock('siamcse21_abs')">abstract</a> |
            <a href="https://drive.google.com/file/d/1XZfx187wEOQI5lvzev-sYabdoE2HR2u5/view">poster</a> |
    
            <p align="justify">
                <i id='siamcse21_abs'>
                  Spectral indices are combinations of the pixel values from two or more spectral bands in a multispectral image.
                  Spectral indices are used to highlight pixels showing the relative abundance or lack of a land-cover type of interest in an image.
                  This study aims to build a feature space with some spectral indices and see if those indices are useful and efficient for the classification of satellite images.
                  The training data is extracted from the NAIP program with six classes (building, barren land, trees, grassland, road, and water).
                  Each image has four spectral bands (RED, GREEN, BLUE, and NIR). After some comparison and analysis, the spectral indices used are Modified Soil-adjusted Vegetation Index (MSAVI), Atmospherically Resistant Vegetation Index (ARVI), Normalized Difference Water Index (NDWI), Difference Spectral Building Index (DSBI), and Road Extraction Index (REI).
                  With these spectral indices and the distribution of the four spectral bands (here, we use mean and standard deviation to represent each distribution), an 18-dimensional feature space is formed.
                  The dimensionality of the feature space is then reduced to improve the classification accuracy using the Unsupervised Feature Selection with Ordinal Locality (UFSOL).
                  The spectral indices have performed a good job as features and has attained 92.58% of accuracy.
                </i>
            </p>
            </div>
        </td>
    </tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
        <tr><td><sectionheading>&nbsp;&nbsp;Projects</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
    <tr>
        <td width="50%" valign="top" align="center">
            <img src="images/dior_vton.jpg" alt="sym" width="320" height="200" style="border-radius:15px">
        </td>

        <td width="50%" valign="top">
            <p>
                <a href="#" id="dior-vton"></a>
                <heading>Learning Virtual Try-on for Image-based Tasks</heading>
                <br>
                 An implementation of image-based virtual try-on using <a href="https://arxiv.org/abs/2104.07021">DiOr</a> as the base pipeline and <a href="https://arxiv.org/abs/2204.01046">Parser-Based Appearance Flow Style</a> as the flow field estimator.
                <br>
                 | <a href="https://github.com/shaoyuc3/dior-VTON">code</a> |
            </p>
        </td>
    </tr>

    <tr>
        <td width="50%" valign="top" align="center">
            <img src="images/ray-tracing.png" alt="sym" width="300" height="200" style="border-radius:15px">
        </td>
        <td width="50%" valign="top">
            <p>
                <heading>Python-RayTracing</heading>
                <br>
                A Python implementation of Ray Tracing based on <a href="https://raytracing.github.io/">C++ Ray Tracing Book Series</a> by Peter Shirley.
                <br>
                | <a href="https://github.com/shaoyuc3/Python-RayTracing">code</a> |
            </p>
        </td>
    </tr>
  
    <tr>
        <td width="50%" valign="top" align="center">
            <img src="images/blindDeconvolution.png" alt="sym" width="300" height="180" style="border-radius:15px">
        </td>

        <td width="50%" valign="top">
            <p>
                <a href="#" id="blindDeconvlotion"></a>
                <heading>Blind Deconvolution with Image Statistics</heading>
                <br>
                 Built a model to recover blurred satellite images by simulating histograms of motion blurring kernels in different degree angles and lengths.
                <br>
                 | <a href="https://drive.google.com/file/d/19Rr8FN8ZE_hQUU74or9FROtMCUPSRdO5/view">poster</a> |
            </p>
        </td>
      </tr>

<table width="100%" align="center" border="0" cellpadding="10">
  <tr>
    <td>
      <sectionheading>&nbsp;&nbsp;Awards</sectionheading>
        <ul>
          <li>Excellence Fellowship, SBU</li>
          <li>Mathematics Presidential Award for Outstanding Performance in Discrete Mathematics, NCTU</li>
        </ul>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr><td><br><p align="right"><font size="2">
    Thanks for this <a href="https://yccyenchicheng.github.io/">template</a>.
    </font></p></td></tr>
    
</table>
  </td> </tr>
</table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
  
<script xml:space="preserve" language="JavaScript">
hideblock('controllable_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('initialization_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('DiffusionAtlas_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('ACDG-VTON_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('handwriting_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('siamcse21_abs');
</script>

</script>
</body>

</html>
